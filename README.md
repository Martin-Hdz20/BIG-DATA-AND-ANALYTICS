<h1>Introducción</h1>
En la actualidad existen una gran cantidad de usuarios conectados a internet, por lo que conlleva a generar mucha información que necesita ser procesada para tener un mejor control de la información de cada persona. Actualmente el área de informática en especial la tecnología inteligente y digital están más enfocados en las etapas de almacenamiento como lo es la gestión, muestreo de comunicaciones y el consumo de información. Esto conduce sobre el tratamiento que tiene la información al ser una gran cantidad de información que se recibe se necesita de mayor seguridad para evitar pérdidas de información. 

Fondo
Investigadores personas con conocimiento a la tecnología mencionan que la BigData no es un lenguaje, es un dato cuyo tamaño es grande, por lo que se puede medir en Bytes, Kilobytes, etc. Pero no es así, debido a tantos dispositivos conectados al internet (IoT) se genera una gran demanda de información de diferentes dispositivos por lo que los sistemas no están capacitados para el tratamiento de grandes cantidades de información. Se crean nuevos sistemas para el tratamiento y almacenamiento de esta información, pero tienen una desventaja que es el tiempo que no permite dar a conocer información en tiempo real debido al gran volumen que hay de información almacenada. 
Volumen de datos
Es la cantidad de información que se genera desde la fuente hasta llegar al almacenamiento.
Velocidad de datos 
Es la velocidad en que los datos viajan para ser almacenados.
Veracidad de datos
Existen 3 formas de cómo se recopilan los datos que son:
•	Datos estructurados.
•	Datos no estructurados.
•	Datos semiestructurados.

Veracidad de los datos
El Big Data reúne las 4 V como el conjunto de datos que tiene un alto volumen, velocidad, variedad y velocidad de conjuntos de datos. Estos conjuntos de datos suelen ser tan grandes y complejos que se vuelven difíciles de procesar.

Arquitectura 
Arquitectura de los datos tradicional
Para esta arquitectura se utiliza datos estructurados, ubicado en el almacenamiento lo que permite generar consultas, la arquitectura tradicional ocupa la propiedad de ACID donde las cifras significan Atomicidad, Consistencia, Aislamiento y Durabilidad. Una de las cualidades de ACID es mantener y asegurar la integridad de los datos.
Arquitectura BigData 
Está enfocado en el almacenamiento de datos estructurados, no estructurados además que es rápida y admite mucha demanda. Para poder realizar las consultas de una forma fácil y eficiente se utiliza la tecnología Hadoop. La tecnología Hadoop utiliza clústeres lo que ayuda a procesar los datos rápidamente.  Cabe mencionar que BigData utiliza de 6 técnicas para el análisis de datos las cuales son:
  •	Aprendizaje de reglas de asociación
  •	Análisis de CART
  •	Análisis de regresión
  •	Aprendizaje automático
  •	Análisis de los sentimientos
  •	Análisis de redes sociales
  •	Aprendizaje de reglas de asociación

Aprendizaje de reglas de asociación 
Esta técnica es una rutina para descubrir correlaciones entre variables en grandes bases de datos. Este método utiliza el algoritmo a priori, a priori significa usar información previa para predecir el patrón futuro. La técnica se está utilizando para descubrir cómo aumentar las ventas, extraer información del registro del servidor web, monitorear los registros del sistema, analizar información biológica, la probabilidad de comprar producción alternativa, etc.
Análisis CART
La clasificación estadística es una estrategia para reconocer clasificaciones en las que otra percepción tiene. Requiere un conjunto de preparación de percepciones efectivamente reconocidas, información registrada, la clasificación estadística se utiliza para asignar documentos a grupos, separar el conjunto de datos en clases, es decir, sí o no. En caso de que la variable objetivo tenga más de dos clases, se utiliza el algoritmo variante C4.5 y, sin embargo, para desgloses binarios, se utiliza el proceso CART estándar.
Análisis de regresión
Esta técnica trata principalmente con conjuntos de datos estadísticos, para encontrar curvas en los datos. La regresión lineal múltiple es una extensión del análisis de regresión lineal simple donde trabajamos con múltiples variables independientes.
Aprendizaje automático
Este proceso consiste en un software que puede recoger datos, además brinda al sistema la capacidad de retomar sin depender de la programación basada en reglas y se enfoca en crear predicciones basadas en datos de entrenamiento.
El aprendizaje automático se basa en un proceso de cinco pasos:
  •	Recopilación de datos: en este paso identificamos las fuentes de datos, recopilamos los datos de ellas y finalmente integramos los datos de estas múltiples fuentes.
  •	Preparación de datos: este proceso está segmentado en dos subprocesos 1. Exploración de datos, en este proceso hacemos un análisis preliminar y entendemos la naturaleza de       los datos para un resultado exitoso y 2.
  •	Análisis de datos: Bueno, aquí este enfoque implica seleccionar las técnicas analíticas, construir el modelo utilizando los datos y evaluar los resultados.
  •	Resultados / Informe: En este nivel presenta los resultados de manera comprensible para comunicar los resultados a los usuarios finales con la ayuda de resultados de valor       agregado con respecto a las metas establecidas para el propósito.
  •	Acción: En este paso aplicamos los resultados generados en el nivel 4.
  •	Análisis de los sentimientos
  •	También se llama extracción de conclusiones, que determina la suposición o el estado de ánimo. Un caso típico de uso de esta innovación es averiguar cómo se sienten las personas sobre un tema específico. Las herramientas de análisis de opinión de Lexalytics pueden estar destinadas a elegir la opinión en una variedad de niveles. Donde se califica la estimación en un nivel de, sin embargo, también calificaremos el sentimiento de palabras o expresiones individuales en el documento.
  •	Análisis de Redes Sociales 
  •	El análisis de redes sociales depende de la presunción de la criticidad de las asociaciones entre las unidades comunicantes. La perspectiva de la red social incluye hipótesis, modelos y aplicaciones que se imparten de manera similar a los pensamientos o estrategias sociales.

Sugerencias para futuras investigaciones
La extracción de información significativa a partir de una gran inundación de datos es un tema básico en Big Data. Desde el punto de vista de la seguridad, las principales preocupaciones de Big Data son la privacidad, la integridad y la accesibilidad en cuanto a la información subcontratada. Mucha información se almacena en la nube. Por lo tanto, la investigación adicional es importante para mejorar la competencia de la evaluación de la integridad en la web y, además, la demostración, la investigación y la capacidad de Big Data.
Conclusión
Como aprendimos existen una gran ventaja sobre el tratamiento de la información, además que existen procesos también existen herramientas como lo es Hadoop el cual permite tener un mejor control de información en grandes volúmenes lo cual tiene como ventaja la consulta en tiempo real de grandes cantidades de información, para dar a conocer consultas de grandes cantidades es necesario tener buenas herramientas que permitan dar a conocer los datos más relevantes. 

